{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['，', '的', '。', '、', '在', '了', '是', '\\u3000', '“', '”', '和', '年', '月', '：', '也', '）', '为', '（', '有', '%', '日', '将', '中', '-', '到', '与', '对', ':', '\\xa0', '上', '都', '等', '不', '他', '》', '《', '就', '但', '我', '而', '这', '会', '并', '；', '被', '后', '人', '从', '还', '1', '3', '6', '以', '新', '说', '7', '2', '要', '5', '？', '更', '于', '个', '10', '大', '时', '4', '多', '/', '让', '其', ')', '(', '很', '及', '下', '', '能', '—', '或', '该', '她', '比', '8', '元', '12', '已', '向', '做', '来', '前', '由', '好', '.', '称', '给', '最', '11', '·', '据', '着', '又', '至', '9', '20', '！', '[', ']', '去', '可', '把', '则', '', '一', '地', '高', '吗', '30', '所', '分', '较', '内', '第', '里', '占', '过', '15', '曾', '\"', '再', '人民日报', '新闻网', '它', '况', '而且', '而是', '而外', '而言', '而已', '尔后', '反过来', '反过来说', '反之', '非但', '非徒', '否则', '嘎', '嘎登', '该', '赶', '个', '各', '各个', '各位', '各种', '各自', '给', '根据', '跟', '故', '故此', '固然', '关于', '管', '归', '果然', '果真', '过', '哈', '哈哈', '呵', '和', '何', '何处', '何况', '何时', '嘿', '哼', '哼唷', '呼哧', '乎', '哗', '还是', '还有', '换句话说', '换言之', '或', '或是', '或者', '极了', '及', '及其', '及至', '即', '即便', '即或', '即令', '即若', '即使', '几', '几时', '己', '既', '既然', '既是', '继而', '加之', '假如', '假若', '假使', '鉴于', '将', '较', '较之', '叫', '接着', '结果', '借', '紧接着', '进而', '尽', '尽管', '经', '经过', '就', '就是', '就是说', '据', '具体地说', '具体说来', '开始', '开外', '靠', '咳', '可', '可见', '可是', '可以', '况且', '啦', '来', '来着', '离', '例如', '哩', '连', '连同', '两者', '了', '临', '另', '另外', '另一方面', '论', '嘛', '吗', '慢说', '漫说', '冒', '么', '每', '每当', '们', '莫若', '某', '某个', '某些', '拿', '哪', '哪边', '哪儿', '哪个', '哪里', '哪年', '哪怕', '哪天', '哪些', '哪样', '那', '那边', '那儿', '那个', '那会儿', '那里', '那么', '那么些', '那么样', '那时', '那些', '那样', '乃', '乃至', '呢', '能', '你', '你们', '您', '宁', '宁可', '宁肯', '宁愿', '哦', '呕', '啪达', '旁人', '呸', '凭', '凭借', '其', '其次', '其二', '其他', '其它', '其一', '其余', '其中', '起', '起见', '起见', '岂但', '恰恰相反', '前后', '前者', '且', '然而', '然后', '然则', '让', '人家', '任', '任何', '任凭', '如', '如此', '如果', '如何', '如其', '如若', '如上所述', '若非', '若是', '啥', '上下', '尚且', '设若', '设使', '甚而', '甚么', '甚至', '省得', '时候', '什么', '什么样', '使得', '是', '是的', '首先', '谁', '谁知', '顺', '顺着', '似的', '虽', '虽然', '虽说', '虽则', '随', '随着', '所', '所以', '他', '他们', '他人', '它', '它们', '她', '她们', '倘', '倘或', '倘然', '倘若', '倘使', '腾', '替', '通过', '同', '同时', '哇', '万一', '往', '望', '为', '为何', '为了', '为什么', '为着', '喂', '嗡嗡', '我', '我们', '呜', '呜呼', '乌乎', '无论', '无宁', '毋宁', '嘻', '吓', '相对而言', '像', '向', '向着', '嘘', '呀', '焉', '沿', '沿着', '要', '要不', '要不然', '要不是', '要么', '要是', '也', '也罢', '也好', '一', '一般', '一旦', '一方面', '一来', '一切', '一样', '一则', '依', '依照', '矣', '以', '以便', '以及', '以免', '以至', '以至于', '以致', '抑或', '因', '因此', '因而', '因为', '哟', '用', '由', '由此可见', '由于', '有', '有的', '有关', '有些', '又', '于', '于是', '于是乎', '与', '与此同时', '与否', '与其', '越是', '云云', '哉', '再说', '再者', '在', '在下', '咱', '咱们', '则', '怎', '怎么', '怎么办', '怎么样', '怎样', '咋', '照', '照着', '者', '这', '这边', '这儿', '这个', '这会儿', '这就是说', '这里', '这么', '这么点儿', '这么些', '这么样', '这时', '这些', '这样', '正如', '吱', '之', '之类', '之所以', '之一', '只是', '只限', '只要', '只有', '至', '至于', '诸位', '着', '着呢', '自', '自从', '自个儿', '自各儿', '自己', '自家', '自身', '综上所述', '总的来看', '总的来说', '总的说来', '总而言之', '总之', '纵', '纵令', '纵然', '纵使', '遵照', '作为', '兮', '呃', '呗', '咚']\n"
     ]
    }
   ],
   "source": [
    "# load Chinese stopwords, 停用词(Stop Words) ，词典译为“电脑检索中的虚字、非检索用字”,\n",
    "# 为节省存储空间和提高搜索效率，搜索引擎在索引页面或处理搜索请求时会自动忽略某些字或词，这些字或词即被称为Stop Words(停用词)。\n",
    "with open('chinese_stopwords.txt','r',encoding='utf-8') as file: \n",
    "    stopwords = [i[:-1] for i in file.readlines()]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89611, 7)\n",
      "      id      author                  source  \\\n",
      "0  89617         NaN  快科技@http://www.kkj.cn/   \n",
      "1  89616         NaN  快科技@http://www.kkj.cn/   \n",
      "2  89615         NaN  快科技@http://www.kkj.cn/   \n",
      "3  89614         NaN                     新华社   \n",
      "4  89613  胡淑丽_MN7479                   深圳大件事   \n",
      "\n",
      "                                             content  \\\n",
      "0  此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/...   \n",
      "1  骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考...   \n",
      "2  此前的一加3T搭载的是3400mAh电池，DashCharge快充规格为5V/4A。\\r\\n...   \n",
      "3    这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\\r\\n   \n",
      "4  （原标题：44岁女子跑深圳约会网友被拒，暴雨中裸身奔走……）\\r\\n@深圳交警微博称：昨日清...   \n",
      "\n",
      "                                             feature  \\\n",
      "0  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"37\"...   \n",
      "1  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"15\"...   \n",
      "2  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"18\"...   \n",
      "3  {\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...   \n",
      "4  {\"type\":\"新闻\",\"site\":\"网易热门\",\"commentNum\":\"978\",...   \n",
      "\n",
      "                           title  \\\n",
      "0           小米MIUI 9首批机型曝光：共计15款   \n",
      "1     骁龙835在Windows 10上的性能表现有望改善   \n",
      "2      一加手机5细节曝光：3300mAh、充半小时用1天   \n",
      "3  葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）   \n",
      "4       44岁女子约网友被拒暴雨中裸奔 交警为其披衣相随   \n",
      "\n",
      "                                                 url  \n",
      "0     http://www.cnbeta.com/articles/tech/623597.htm  \n",
      "1     http://www.cnbeta.com/articles/tech/623599.htm  \n",
      "2     http://www.cnbeta.com/articles/tech/623601.htm  \n",
      "3  http://world.huanqiu.com/hot/2017-06/10866126....  \n",
      "4  http://news.163.com/17/0618/00/CN617P3Q0001875...  \n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "news = pd.read_csv('sqlResult.csv',encoding='gb18030')\n",
    "print(news.shape)\n",
    "print(news.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87054, 7)\n"
     ]
    }
   ],
   "source": [
    "# drop missing data\n",
    "news = news.dropna(subset=['content'])\n",
    "print(news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split words for meanful segmentations\n",
    "import jieba  #Chinese text segmentation,中文的汉字序列切分成有意义的词 https://github.com/fxsjy/jieba\n",
    "def split_text(text):\n",
    "    text = text.replace(' ','').replace('/n','') #data cleaning, compress space and enter\n",
    "    text2 =jieba.cut(text.strip())\n",
    "    # drop stopwords\n",
    "    result = ' '.join([w for w in text2 if w not in stopwords]) # drop stopwords\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/体验版内测，稳定版暂不受影响），以确保工程师可以集中全部精力进行系统优化工作。有人猜测这也是将精力主要用到MIUI 9的研发之中。\r\n",
      "MIUI 8去年5月发布，距今已有一年有余，也是时候更新换代了。\r\n",
      "当然，关于MIUI 9的确切信息，我们还是等待官方消息。\r\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.776 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此外 本周 除 小米 手机 款 机型 外 机型 暂停 更新 发布 含 开发 版 体验版 内测 稳定版 暂不受 影响 确保 工程师 集中 全部 精力 进行 系统优化 工作 有人 猜测 精力 主要 用到 MIUI9 研发 之中 \r\n",
      " MIUI8 去年 发布 距今已有 一年 有余 更新换代 \r\n",
      " 当然 MIUI9 确切 信息 等待 官方消息\n"
     ]
    }
   ],
   "source": [
    "print(news.iloc[0].content)\n",
    "print(split_text(news.iloc[0].content))  #show the results of the first record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split words using all corpus  \n",
    "corpus = list(map(split_text,[str(i) for i in news.content])) #map:applies a function to all the items in an input_list. \n",
    "print(corpus[2])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes a few min to clean and split all the data, save pickle file for future\n",
    "import pickle,os\n",
    "if not os.path.exists('corpus.pkl'):\n",
    "     # if no available pickle\n",
    "    with open('corpus.pkl','wb') as file: # wb: file is opened for writing in binary mode.\n",
    "        pickle.dump(corpus,file)\n",
    "    \n",
    "else:\n",
    "    with open('corpus.pkl','rb') as file: #\"rb\" mode opens the file in binary format for reading, \n",
    "        corpus = pickle.load(file)  #reak pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute corpus matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "countvectorizer = CountVectorizer(encoding ='gb18030',min_df=0.015) #ignore terms that appear in less than 1.5% of the documents\"\n",
    "tfidftransformer = TfidfTransformer() # tfidf: term frequency–inverse document frequency\n",
    "\n",
    "# tf, then idf =TFIDF\n",
    "countvector = countvectorizer.fit_transform(corpus)\n",
    "tfidf = tfidftransformer.fit_transform(countvector)  # take around 10 to run w/o pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label if its xinhua's news, xinhua=1\n",
    "label = list(map(lambda source:1 if '新华' in str(source) else 0, news.source))\n",
    "\n",
    "# split and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf.toarray(),label,test_size=0.3) #usng 30% data for prediction\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n",
    "y_predict = clf.predict(x_test)\n",
    "\n",
    "\n",
    "# use model to test plagiarism and forcast styles\n",
    "prediction = clf.predict(tfidf.toarray()) # prediction=1, is considered as xinhua style\n",
    "labels =np.array(label) # lable=1, real xinhua \n",
    "\n",
    "# compare news index: two columns, prediction and labels=reality \n",
    "compare_news_index =pd.DataFrame({'prediction':prediction, 'labels':labels})\n",
    "\n",
    "copy_news_index = compare_news_index[(compare_news_index['prediction']==1) & (compare_news_index['labels']==0)].index\n",
    "# xinhua news\n",
    "xinhuashe_news_index = compare_news_index[(compare_news_index['labels']==1)].index\n",
    "print('possible plagiarism (copy)',len(copy_news_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use kmeans for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans  \n",
    "# kmeans only use Euclidean distance, but if we normalize to unit, then cosine distance equivalent to E-distance (revsersely)\n",
    "\n",
    "normalizer = Normalizer()\n",
    "scaled_array = normalizer.fit_transform(tfidf.toarray())\n",
    "\n",
    "# 10 is the number of buskets\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "k_labels = kmeans.fit_predict(scaled_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find top 10 similar news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create id class for styles\n",
    "id_class = {index:class_ for index, class_ in enumerate(k_labels)}\n",
    "\n",
    "from collections import defaultdict\n",
    "class_id = defaultdict(set)\n",
    "\n",
    "for index, class_ in id_class.items():\n",
    "    # class_id from xinhua only\n",
    "    if index in xinhuashe_news_index.tolist():\n",
    "        class_id[class_].add(index)\n",
    "        \n",
    "#output by class\n",
    "#search similar text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def find_similar_text(cpindex,top=10): #only search from xinhua news, class_id key=class, value=id\n",
    "    dist_dict = {i:cosine_similarity(tfidf[cpindex],tfidf[i]) for i in class_id[id_class[cpindex]]} \n",
    "    # sort from smallest to largest\n",
    "    return sorted(dist_dict.items(), key=lambda x:x[1][0],reverse=True)[:top]\n",
    "\n",
    "# test, select an article number from the 89k articles, cpindex is the subscript\n",
    "cpindex =3352\n",
    "\n",
    "similar_list = find_similar_text(cpindex)\n",
    "print(similar_list)\n",
    "print('possible plagiarism articles:\\n',news_iloc[cpindex].content)\n",
    "# find a similar original news\n",
    "similar2 = simiar_list[0][0]\n",
    "print('similar original:\\n', news.iloc[similar2].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
